{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge2_goUbsL0Q"
   },
   "source": [
    "# EVA Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Tuxf9JkqUbHM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from math import sqrt,pi,exp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import find_peaks,find_peaks_cwt\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##code for mol3dfile\n",
    "\n",
    "\n",
    "# parent_directory = 'C:/Users/skdha/Downloads/Olfaction_project/irchracterizationcnn--main (3)/irchracterizationcnn--main/nist_dataset/mol_3d/'\n",
    "# subdirectory_names = [d for d in os.listdir(parent_directory) if os.path.isdir(os.path.join(parent_directory, d))]\n",
    "\n",
    "# def file_generate(cas_number):\n",
    "#     file_path = parent_directory+cas_number+\"/downloaded_files/mol3D.cgi\"\n",
    "#     if not os.path.exists(file_path):\n",
    "#         return None\n",
    "    \n",
    "#     # Replace 'cgi_file_path' with the actual path to your CGI file\n",
    "#     cgi_file_path = file_path\n",
    "\n",
    "#     # Read the file\n",
    "#     with open(cgi_file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     # Initialize variables to store the IR frequency data\n",
    "#     ir_data = []\n",
    "\n",
    "#     # Find the start and end lines of the IR frequency data\n",
    "#     start_line = None\n",
    "#     end_line = None\n",
    "#     for idx, line in enumerate(lines):\n",
    "#         if \"> <IR.FREQUENCIES>\" in line:\n",
    "#             start_line = idx + 2  # Skip header line\n",
    "#         elif start_line is not None and line.strip() == '':\n",
    "#             end_line = idx \n",
    "#             break\n",
    "\n",
    "#     # Extract IR frequency data\n",
    "#     if start_line is not None and end_line is not None:\n",
    "#         for line in lines[start_line:end_line]:\n",
    "#             freq, intensity = map(float, line.strip().split())\n",
    "#             ir_data.append({'Frequency': freq, 'Intensity': intensity})\n",
    "\n",
    "#     # Create a pandas DataFrame from the extracted data\n",
    "#     ir_df = pd.DataFrame(ir_data,columns=None)\n",
    "#     return ir_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for sdf file\n",
    "\n",
    "parent_directory = 'sdf/'\n",
    "sdf_files = [file for file in os.listdir(parent_directory) if file.endswith('.sdf')]\n",
    "\n",
    "subdirectory_names= []\n",
    "for i in sdf_files:\n",
    "    subdirectory_names.append(i.split(\".sdf\")[0])\n",
    "    \n",
    "\n",
    "def file_generate(cas_number):\n",
    "    file_path = parent_directory+cas_number+\".sdf\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    \n",
    "    # Replace 'cgi_file_path' with the actual path to your CGI file\n",
    "    cgi_file_path = file_path\n",
    "\n",
    "    # Read the file\n",
    "    with open(cgi_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize variables to store the IR frequency data\n",
    "    ir_data = []\n",
    "\n",
    "    # Find the start and end lines of the IR frequency data\n",
    "    start_line = None\n",
    "    end_line = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \"> <IR.FREQUENCIES>\" in line:\n",
    "            start_line = idx + 2  # Skip header line\n",
    "        elif start_line is not None and line.strip() == '':\n",
    "            end_line = idx \n",
    "            break\n",
    "\n",
    "    # Extract IR frequency data\n",
    "    if start_line is not None and end_line is not None:\n",
    "        for line in lines[start_line:end_line]:\n",
    "            freq, intensity = map(float, line.strip().split())\n",
    "            ir_data.append({'Frequency': freq, 'Intensity': intensity})\n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    ir_df = pd.DataFrame(ir_data,columns=None)\n",
    "    return ir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas_no = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(subdirectory_names)):\n",
    "    file_name = file_generate(subdirectory_names[i])\n",
    "   \n",
    "    if (not isinstance(file_name,pd.DataFrame)) or file_name.empty:\n",
    "        print(subdirectory_names[i])\n",
    "        continue\n",
    "    cas_no.append(subdirectory_names[i])\n",
    "    x_values = file_name['Frequency']\n",
    "    y_values = file_name['Intensity']\n",
    "    x = np.around(x_values).astype(int)\n",
    "    new_x = list(range(1,4001))\n",
    "    \n",
    "    # Create the old DataFrame\n",
    "    old_data = {'common_column': x, 'value_column': y_values}\n",
    "    old_df = pd.DataFrame(old_data)\n",
    "\n",
    "    # Create the new DataFrame with the common column values you want\n",
    "    new_data = {'common_column': new_x}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Merge the new DataFrame with the old DataFrame based on the common column\n",
    "    merged_df = new_df.merge(old_df, on='common_column', how='left')\n",
    "\n",
    "    # Fill missing values with zeros\n",
    "    merged_df['value_column'] = merged_df['value_column'].fillna(0)\n",
    "    \n",
    "    smoothed = []\n",
    "\n",
    "    # gaussian function\n",
    "    def gaussian_kernel(x, mean, sigma):\n",
    "        return np.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "\n",
    "    for i in merged_df['common_column'].to_list():\n",
    "        kv = gaussian_kernel(merged_df['common_column'],i,10)\n",
    "    #     kv /= kv.sum()\n",
    "        smoothed.append((merged_df['value_column']*kv).sum())\n",
    "\n",
    "\n",
    "    # # Calculate the sum of the list\n",
    "    max_val = max(smoothed)\n",
    "\n",
    "    # # Normalize the list\n",
    "    normalized_list = [x / max_val for x in smoothed]\n",
    "    \n",
    "        # now produce features for the model\n",
    "    # Reshape the array into a 2D array of shape (num_rows, 5)\n",
    "\n",
    "    elements_to_remove = len(normalized_list) % 5\n",
    "\n",
    "    # Use slicing to remove elements from the beginning of the list\n",
    "    normalized_list = normalized_list[elements_to_remove:]\n",
    "\n",
    "    reshaped_array = np.array(normalized_list).reshape(-1, 5)\n",
    "\n",
    "    # Calculate the mean along the rows\n",
    "    eva_des = np.mean(reshaped_array, axis=1)\n",
    "    \n",
    "    # Check if the list length exceeds 600\n",
    "    if len(eva_des) > 4000:\n",
    "        excess = len(eva_des) - 4000  # Calculate the number of excess elements\n",
    "\n",
    "        # Trim the list by removing 'excess' elements from both the beginning and end\n",
    "        trimmed_list = eva_des[excess // 2:-excess // 2]\n",
    "\n",
    "        # Now 'trimmed_list' contains at most 600 elements\n",
    "    else:\n",
    "        trimmed_list = eva_des  # If the list is already 600 elements or less, no need to trim\n",
    "\n",
    "\n",
    "    # trimmed list is features list\n",
    "  \n",
    "    feature_vectors.append(trimmed_list)\n",
    "# column_names = ['CAS_Number', 'Feature2', 'Feature3', 'Feature4', 'Feature5']\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(feature_vectors, columns=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc = 0,\n",
    "          column = 'cas_no',\n",
    "          value = cas_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100061\n",
       "Name: cas_no, dtype: int32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cas_no'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mol3d_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature from sdf file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = 'sdf/'\n",
    "\n",
    "# List all the .sdf files in the parent directory\n",
    "sdf_files = [file for file in os.listdir(parent_directory) if file.endswith('.sdf')]\n",
    "cas_no = []\n",
    "feature_vectors_gaussian = []\n",
    "for sdf_file in sdf_files:\n",
    "    cas_number = os.path.splitext(sdf_file)[0]\n",
    "    file_path = os.path.join(parent_directory, sdf_file)\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    ir_data = []\n",
    "    start_line = None\n",
    "    end_line = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \"> <IR.FREQUENCIES>\" in line:\n",
    "            start_line = idx + 2 \n",
    "        elif start_line is not None and line.strip() == '':\n",
    "            end_line = idx\n",
    "            break\n",
    "    try:\n",
    "        if start_line is not None and end_line is not None:\n",
    "            for line in lines[start_line:end_line]:\n",
    "                freq, intensity = map(float, line.strip().split())\n",
    "                ir_data.append({'Frequency': freq, 'Intensity': intensity})\n",
    "        ir_df = pd.DataFrame(ir_data)\n",
    "\n",
    "        # Generate feature vector with all 4000 features\n",
    "        x_values = ir_df['Frequency']\n",
    "        y_values = ir_df['Intensity']\n",
    "        x = np.around(x_values).astype(int)\n",
    "        new_x = list(range(1, 4001))\n",
    "        old_data = {'common_column': x, 'value_column': y_values}\n",
    "        old_df = pd.DataFrame(old_data)\n",
    "        new_data = {'common_column': new_x}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        merged_df = new_df.merge(old_df, on='common_column', how='left')\n",
    "        merged_df['value_column'] = merged_df['value_column'].fillna(0)\n",
    "\n",
    "        # Smooth the intensity values using a Gaussian kernel\n",
    "        smoothed = []\n",
    "        for i in merged_df['common_column'].to_list():\n",
    "            kv = np.exp(-0.5 * ((merged_df['common_column'] - i) / 10) ** 2) / (10 * np.sqrt(2 * np.pi))\n",
    "            smoothed.append((merged_df['value_column'] * kv).sum())\n",
    "\n",
    "        feature_vectors_gaussian.append(smoothed)\n",
    "        cas_no.append(cas_number)\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Skipping file {sdf_file} as 'IR.FREQUENCIES' data is not found.\")\n",
    "df_gaussian = pd.DataFrame(feature_vectors_gaussian)\n",
    "df_gaussian.insert(loc=0, column='cas_no', value=cas_no)\n",
    "df_gaussian.to_csv(\"sdf_data_gaussian.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 'C:\\Users\\91830\\Downloads\\sdf1': 2364\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise ValueError(\"The provided path is not a directory.\")\n",
    "\n",
    "    file_count = 0\n",
    "\n",
    "    for item in os.listdir(directory_path):\n",
    "        item_path = os.path.join(directory_path, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            file_count += 1\n",
    "\n",
    "    return file_count\n",
    "\n",
    "directory_path = r\"C:\\Users\\91830\\Downloads\\sdf1\"\n",
    "try:\n",
    "    count = count_files_in_directory(directory_path)\n",
    "    print(f\"Number of files in '{directory_path}': {count}\")\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory where SDF files are located\n",
    "parent_directory = r\"C:\\Users\\91830\\Downloads\\sdf1\"\n",
    "\n",
    "# Get a list of SDF files in the directory\n",
    "sdf_files = [file for file in os.listdir(parent_directory) if file.endswith('.sdf')]\n",
    "\n",
    "# Initialize lists to store CAS numbers and raw feature data\n",
    "cas_numbers = []\n",
    "raw_features = []\n",
    "\n",
    "# Define a function to read raw data from an SDF file\n",
    "def read_raw_data(file_path):\n",
    "    cas_number = os.path.basename(file_path).split(\".sdf\")[0]\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Initialize variables to store raw data\n",
    "    raw_data = []\n",
    "\n",
    "    # Find the start and end lines of the IR frequency data\n",
    "    start_line = None\n",
    "    end_line = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \"> <IR.FREQUENCIES>\" in line:\n",
    "            start_line = idx + 2  # Skip header line\n",
    "        elif start_line is not None and line.strip() == '':\n",
    "            end_line = idx\n",
    "            break\n",
    "\n",
    "    # Extract IR frequency data\n",
    "    if start_line is not None and end_line is not None:\n",
    "        for line in lines[start_line:end_line]:\n",
    "            freq, intensity = map(float, line.strip().split())\n",
    "            raw_data.append(intensity)  # Append raw intensity values\n",
    "\n",
    "    return cas_number, raw_data\n",
    "\n",
    "# Loop through each SDF file and extract raw data\n",
    "for sdf_file in sdf_files:\n",
    "    file_path = os.path.join(parent_directory, sdf_file)\n",
    "    cas_number, raw_data = read_raw_data(file_path)\n",
    "    \n",
    "    if raw_data:  # Check if raw data is not empty\n",
    "        cas_numbers.append(cas_number)\n",
    "        raw_features.append(raw_data)\n",
    "\n",
    "# Create a DataFrame with CAS numbers and raw features\n",
    "df = pd.DataFrame({'CAS_Number': cas_numbers, 'Raw_Features': raw_features})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"sdf_raw_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "parent_directory = r\"C:\\Users\\91830\\Downloads\\sdf1\"\n",
    "sdf_files = [file for file in os.listdir(parent_directory) if file.endswith('.sdf')]\n",
    "\n",
    "# Define the fixed length for each CAS number\n",
    "fixed_length = 4000\n",
    "cas_numbers = []\n",
    "raw_features_list = []\n",
    "def read_raw_data(file_path):\n",
    "    cas_number = os.path.basename(file_path).split(\".sdf\")[0]\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    raw_data = []\n",
    "    start_line = None\n",
    "    end_line = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if \"> <IR.FREQUENCIES>\" in line:\n",
    "            start_line = idx + 2  # Skip header line\n",
    "        elif start_line is not None and line.strip() == '':\n",
    "            end_line = idx\n",
    "            break\n",
    "    if start_line is not None and end_line is not None:\n",
    "        for line in lines[start_line:end_line]:\n",
    "            freq, intensity = map(float, line.strip().split())\n",
    "            raw_data.append(intensity)  \n",
    "\n",
    "    return cas_number, raw_data\n",
    "for sdf_file in sdf_files:\n",
    "    file_path = os.path.join(parent_directory, sdf_file)\n",
    "    cas_number, raw_data = read_raw_data(file_path)\n",
    "    \n",
    "    if raw_data:  \n",
    "        cas_numbers.append(cas_number)\n",
    "        \n",
    "        # Truncate or zero-pad the raw data to the fixed length\n",
    "        if len(raw_data) < fixed_length:\n",
    "            raw_data.extend([0] * (fixed_length - len(raw_data)))\n",
    "        else:\n",
    "            raw_data = raw_data[:fixed_length]\n",
    "        \n",
    "        raw_features_list.append(raw_data)\n",
    "df = pd.DataFrame({'CAS_Number': cas_numbers})\n",
    "df = pd.concat([df, pd.DataFrame(raw_features_list)], axis=1)\n",
    "df.to_csv(\"sdf_raw_data2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f9e5a49f449f09790208ea0c99ff42a627e312361c1d79484b07944647e521b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
